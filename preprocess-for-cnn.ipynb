{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T22:26:52.928990Z",
     "start_time": "2024-11-12T22:26:52.876842Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField\n",
    "import shutil\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'RANDOM_SEED': 42,\n",
    "    'GPU_ID': 0,\n",
    "    'USE_GPU': True,\n",
    "    'BASE_FOLDER': os.path.join(\"..\", \"input\", \"tlvmc-parkinsons-freezing-gait-prediction\"),\n",
    "    'BATCH_SIZE': 32\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(CONFIG['RANDOM_SEED'])\n",
    "np.random.seed(CONFIG['RANDOM_SEED'])\n",
    "tf.random.set_seed(CONFIG['RANDOM_SEED'])\n",
    "os.environ['PYTHONHASHSEED'] = str(CONFIG['RANDOM_SEED'])\n",
    "\n",
    "# GPU Setup\n",
    "try:\n",
    "    if tf.config.list_physical_devices('GPU') and CONFIG['USE_GPU']:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        # Configure GPU memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        gpu_name = gpus[CONFIG['GPU_ID']].name\n",
    "        device = f'/GPU:{CONFIG[\"GPU_ID\"]}'\n",
    "        logger.info(f\"Successfully configured GPU - {gpu_name}\")\n",
    "    else:\n",
    "        device = '/CPU:0'\n",
    "        logger.info(\"No GPU available or disabled. Using CPU.\")\n",
    "except Exception as e:\n",
    "    device = '/CPU:0'\n",
    "    logger.warning(f\"GPU initialization failed: {str(e)}\")\n",
    "    logger.info(\"Falling back to CPU\")\n",
    "\n",
    "# Set global device variable\n",
    "DEVICE = device\n",
    "\n",
    "N_CPU_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "logger.info(f\"Number of CPU cores available: {N_CPU_CORES}\")\n",
    "logger.info(f\"Base folder: {CONFIG['BASE_FOLDER']}\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .config-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 10px 0;\n",
    "        font-size: 0.9em;\n",
    "        font-family: sans-serif;\n",
    "        min-width: 400px;\n",
    "        box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    .config-table thead tr {\n",
    "        background-color: #009879;\n",
    "        color: #ffffff;\n",
    "        text-align: left;\n",
    "    }\n",
    "    .config-table th,\n",
    "    .config-table td {\n",
    "        padding: 12px 15px;\n",
    "    }\n",
    "    .config-table tbody tr {\n",
    "        border-bottom: 1px solid #dddddd;\n",
    "    }\n",
    "</style>\n",
    "<table class=\"config-table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Configuration</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>Device</td><td>\"\"\" + DEVICE + \"\"\"</td></tr>\n",
    "        <tr><td>CPU Cores</td><td>\"\"\" + str(N_CPU_CORES) + \"\"\"</td></tr>\n",
    "        <tr><td>Random Seed</td><td>\"\"\" + str(CONFIG['RANDOM_SEED']) + \"\"\"</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\"\"\"))\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    pass"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:26:52,919 - INFO - Successfully configured GPU - /physical_device:GPU:0\n",
      "2024-11-13 04:26:52,921 - INFO - Using device: /GPU:0\n",
      "2024-11-13 04:26:52,921 - INFO - Number of CPU cores available: 12\n",
      "2024-11-13 04:26:52,922 - INFO - Base folder: ..\\input\\tlvmc-parkinsons-freezing-gait-prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "<style>\n",
       "    .config-table {\n",
       "        border-collapse: collapse;\n",
       "        margin: 10px 0;\n",
       "        font-size: 0.9em;\n",
       "        font-family: sans-serif;\n",
       "        min-width: 400px;\n",
       "        box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);\n",
       "    }\n",
       "    .config-table thead tr {\n",
       "        background-color: #009879;\n",
       "        color: #ffffff;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .config-table th,\n",
       "    .config-table td {\n",
       "        padding: 12px 15px;\n",
       "    }\n",
       "    .config-table tbody tr {\n",
       "        border-bottom: 1px solid #dddddd;\n",
       "    }\n",
       "</style>\n",
       "<table class=\"config-table\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Configuration</th>\n",
       "            <th>Value</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr><td>Device</td><td>/GPU:0</td></tr>\n",
       "        <tr><td>CPU Cores</td><td>12</td></tr>\n",
       "        <tr><td>Random Seed</td><td>42</td></tr>\n",
       "    </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:26:55.780699Z",
     "start_time": "2024-11-12T22:26:55.753457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reduce_memory_usage(df):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a pandas DataFrame by optimizing data types.\n",
    "    Reference: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 @ARJANGROEN\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Memory-optimized DataFrame\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "        \n",
    "        # Skip datetime and category types\n",
    "        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n",
    "            if (col_type != 'object'):\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                # Integer optimization\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "\n",
    "                # Float optimization\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype('category')\n",
    "                \n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage became: {:.2f} MB\".format(mem_usg))\n",
    "    print(\"Memory reduced by {:.1f}%\".format(100 * (start_mem - mem_usg) / start_mem))\n",
    "    \n",
    "    return df"
   ],
   "id": "4c0ac6660fa6173",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DOWN_SAMPLE = True\n",
    "FROM_FREQ = 128  # Original sampling frequency\n",
    "TO_FREQ = 64    # Target sampling frequency\n",
    "FREQ_RATIO = FROM_FREQ // TO_FREQ\n",
    "\n",
    "def load_tdcsfog_data(base_folder):\n",
    "    \"\"\"\n",
    "    Load and downsample TDCSFOG dataset.\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Base path to the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated and processed dataset\n",
    "    \"\"\"\n",
    "    DATA_ROOT_TDCSFOG = os.path.join(base_folder, 'tdcsfog')\n",
    "    tdcsfog = pd.DataFrame()\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    files_list = []\n",
    "    for root, _, files in os.walk(DATA_ROOT_TDCSFOG):\n",
    "        files_list.extend([os.path.join(root, name) for name in files])\n",
    "    \n",
    "    logger.info(f\"Found {len(files_list)} files to process\")\n",
    "    \n",
    "    for file_path in tqdm(files_list, desc='Loading TDCSFOG data'):\n",
    "        try:\n",
    "            df_list = pd.read_csv(file_path)\n",
    "            file_name = os.path.basename(file_path).split('.')[0]\n",
    "            \n",
    "            if DOWN_SAMPLE:\n",
    "                df_list = df_list.groupby(np.arange(len(df_list)) // FREQ_RATIO).max()\n",
    "            \n",
    "            df_list['file'] = file_name\n",
    "            \n",
    "            tdcsfog = pd.concat([tdcsfog, df_list], axis=0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Loaded dataset with shape: {tdcsfog.shape}\")\n",
    "    return tdcsfog\n",
    "\n",
    "BASE_FOLDER = 'data/csv'\n",
    "# Load the data\n",
    "tdcsfog = load_tdcsfog_data(BASE_FOLDER)"
   ],
   "id": "b63b27de067235e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:28:48.649478Z",
     "start_time": "2024-11-12T22:28:48.624838Z"
    }
   },
   "cell_type": "code",
   "source": "display(tdcsfog.head())",
   "id": "91c72c1330a09c24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Time      AccV     AccML     AccAP  StartHesitation  Turn  Walking  \\\n",
       "0     1 -9.533939  0.566322 -1.413525                0     0        0   \n",
       "1     3 -9.529345  0.564227 -1.415490                0     0        0   \n",
       "2     5 -9.536585  0.561854 -1.413949                0     0        0   \n",
       "3     7 -9.524494  0.552772 -1.413802                0     0        0   \n",
       "4     9 -9.529338  0.552960 -1.415914                0     0        0   \n",
       "\n",
       "         file  \n",
       "0  003f117e14  \n",
       "1  003f117e14  \n",
       "2  003f117e14  \n",
       "3  003f117e14  \n",
       "4  003f117e14  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>AccV</th>\n",
       "      <th>AccML</th>\n",
       "      <th>AccAP</th>\n",
       "      <th>StartHesitation</th>\n",
       "      <th>Turn</th>\n",
       "      <th>Walking</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-9.533939</td>\n",
       "      <td>0.566322</td>\n",
       "      <td>-1.413525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-9.529345</td>\n",
       "      <td>0.564227</td>\n",
       "      <td>-1.415490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-9.536585</td>\n",
       "      <td>0.561854</td>\n",
       "      <td>-1.413949</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>-9.524494</td>\n",
       "      <td>0.552772</td>\n",
       "      <td>-1.413802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>-9.529338</td>\n",
       "      <td>0.552960</td>\n",
       "      <td>-1.415914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tdcsfog = reduce_memory_usage(tdcsfog)",
   "id": "cec0fef27bf92a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:29:23.027878Z",
     "start_time": "2024-11-12T22:29:21.138033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_merged_tdcsfog_with_meta_data(tdcsfog: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge TDCSFOG data with metadata and sort by subject and timestamp.\n",
    "    \n",
    "    Args:\n",
    "        tdcsfog (pd.DataFrame): The main TDCSFOG dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged and sorted dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata_path = os.path.join(BASE_FOLDER, \"tdcsfog_metadata.csv\")\n",
    "        tdcsfog_metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        logger.info(f\"Metadata shape before merge: {tdcsfog_metadata.shape}\")\n",
    "        logger.info(f\"TDCSFOG shape before merge: {tdcsfog.shape}\")\n",
    "        \n",
    "        tdcsfog_merged = tdcsfog_metadata.merge(\n",
    "            tdcsfog, \n",
    "            how='inner', \n",
    "            left_on='Id', \n",
    "            right_on='file',\n",
    "            validate='1:m'  # Validate one-to-many relationship\n",
    "        )\n",
    "        \n",
    "        # Remove redundant column\n",
    "        tdcsfog_merged.drop(columns=['file'], axis=1, inplace=True)\n",
    "        \n",
    "        # Sort by specified columns\n",
    "        tdcsfog_merged = tdcsfog_merged.sort_values(\n",
    "            by=['Id', 'Subject', 'Time'],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Final merged shape: {tdcsfog_merged.shape}\")\n",
    "        \n",
    "        # Verify no data was unexpectedly lost\n",
    "        expected_rows = len(tdcsfog)\n",
    "        if len(tdcsfog_merged) != expected_rows:\n",
    "            logger.warning(\n",
    "                f\"Row count mismatch! Expected {expected_rows}, got {len(tdcsfog_merged)}\"\n",
    "            )\n",
    "            \n",
    "        return tdcsfog_merged\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Metadata file not found at {metadata_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during merge operation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    tdcsfog_merged = get_merged_tdcsfog_with_meta_data(tdcsfog)\n",
    "    display(tdcsfog_merged.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge data: {str(e)}\")"
   ],
   "id": "d7f29e0e245514b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:29:21,184 - INFO - Metadata shape before merge: (833, 5)\n",
      "2024-11-13 04:29:21,184 - INFO - TDCSFOG shape before merge: (3531552, 8)\n",
      "2024-11-13 04:29:22,996 - INFO - Final merged shape: (3531552, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           Id Subject  Visit  Test Medication  Time      AccV     AccML  \\\n",
       "0  003f117e14  4dc2f8      3     2         on     1 -9.533939  0.566322   \n",
       "1  003f117e14  4dc2f8      3     2         on     3 -9.529345  0.564227   \n",
       "2  003f117e14  4dc2f8      3     2         on     5 -9.536585  0.561854   \n",
       "3  003f117e14  4dc2f8      3     2         on     7 -9.524494  0.552772   \n",
       "4  003f117e14  4dc2f8      3     2         on     9 -9.529338  0.552960   \n",
       "\n",
       "      AccAP  StartHesitation  Turn  Walking  \n",
       "0 -1.413525                0     0        0  \n",
       "1 -1.415490                0     0        0  \n",
       "2 -1.413949                0     0        0  \n",
       "3 -1.413802                0     0        0  \n",
       "4 -1.415914                0     0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Test</th>\n",
       "      <th>Medication</th>\n",
       "      <th>Time</th>\n",
       "      <th>AccV</th>\n",
       "      <th>AccML</th>\n",
       "      <th>AccAP</th>\n",
       "      <th>StartHesitation</th>\n",
       "      <th>Turn</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>4dc2f8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.533939</td>\n",
       "      <td>0.566322</td>\n",
       "      <td>-1.413525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>4dc2f8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>on</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.529345</td>\n",
       "      <td>0.564227</td>\n",
       "      <td>-1.415490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>4dc2f8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>on</td>\n",
       "      <td>5</td>\n",
       "      <td>-9.536585</td>\n",
       "      <td>0.561854</td>\n",
       "      <td>-1.413949</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>4dc2f8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>on</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.524494</td>\n",
       "      <td>0.552772</td>\n",
       "      <td>-1.413802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>4dc2f8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>on</td>\n",
       "      <td>9</td>\n",
       "      <td>-9.529338</td>\n",
       "      <td>0.552960</td>\n",
       "      <td>-1.415914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:29:30.705507Z",
     "start_time": "2024-11-12T22:29:30.689558Z"
    }
   },
   "cell_type": "code",
   "source": "tdcsfog_m = tdcsfog_merged",
   "id": "739ea6057a20c2fc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:29:33.589562Z",
     "start_time": "2024-11-12T22:29:32.155248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_event_and_label(df_input: pd.DataFrame, drop_unnecessary: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess event and label columns in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df_input (pd.DataFrame): Input dataframe\n",
    "        drop_unnecessary (bool): Whether to drop original event columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with encoded events and labels\n",
    "    \"\"\"\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    \n",
    "    MEDICATION_MAP = {'off': 0, 'on': 1}\n",
    "    LABEL_MAP = {\n",
    "        'Normal': 0,\n",
    "        'StartHesitation': 1,\n",
    "        'Turn': 2,\n",
    "        'Walking': 3\n",
    "    }\n",
    "    EVENT_MAP = {\n",
    "        'Normal': 0,\n",
    "        'StartHesitation': 1,\n",
    "        'Turn': 1,\n",
    "        'Walking': 1\n",
    "    }\n",
    "    \n",
    "    conditions = [\n",
    "        (df['StartHesitation'] == 1),\n",
    "        (df['Turn'] == 1),\n",
    "        (df['Walking'] == 1)\n",
    "    ]\n",
    "    choices = ['StartHesitation', 'Turn', 'Walking']\n",
    "    \n",
    "    df['Event'] = np.select(conditions, choices, default='Normal')\n",
    "    df['Label'] = df['Event'].copy()\n",
    "    \n",
    "    df['Medication'] = df['Medication'].map(MEDICATION_MAP)\n",
    "    df['Label'] = df['Label'].map(LABEL_MAP)\n",
    "    df['Event'] = df['Event'].map(EVENT_MAP)\n",
    "    \n",
    "    # Display value distributions with gradient\n",
    "    logger.info(\"Label distribution:\")\n",
    "    display(df['Label'].value_counts().to_frame().style.background_gradient())\n",
    "    \n",
    "    logger.info(\"Event distribution:\")\n",
    "    display(df['Event'].value_counts().to_frame().style.background_gradient())\n",
    "    \n",
    "    # Drop unnecessary columns if required\n",
    "    if drop_unnecessary:\n",
    "        cols_to_drop = ['StartHesitation', 'Turn', 'Walking']\n",
    "        df.drop(columns=cols_to_drop, axis=1, inplace=True)\n",
    "        logger.info(f\"Dropped columns: {cols_to_drop}\")\n",
    "    \n",
    "    # Verify data integrity\n",
    "    assert df['Event'].isin([0, 1]).all(), \"Invalid values in Event column\"\n",
    "    assert df['Label'].isin([0, 1, 2, 3]).all(), \"Invalid values in Label column\"\n",
    "    assert df['Medication'].isin([0, 1]).all(), \"Invalid values in Medication column\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "try:\n",
    "    processed_df = preprocess_event_and_label(df_input=tdcsfog_merged, drop_unnecessary=True)\n",
    "    logger.info(f\"Processed dataframe shape: {processed_df.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during preprocessing: {str(e)}\")"
   ],
   "id": "adec079575a03023",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:29:33,354 - INFO - Label distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x227ad746fe0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8138e_row0_col0 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8138e_row1_col0 {\n",
       "  background-color: #bbc7e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8138e_row2_col0 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8138e_row3_col0 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8138e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8138e_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Label</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8138e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8138e_row0_col0\" class=\"data row0 col0\" >2435287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8138e_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_8138e_row1_col0\" class=\"data row1 col0\" >839866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8138e_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "      <td id=\"T_8138e_row2_col0\" class=\"data row2 col0\" >152442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8138e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8138e_row3_col0\" class=\"data row3 col0\" >103957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:29:33,401 - INFO - Event distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x227acb81810>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1b868_row0_col0 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1b868_row1_col0 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1b868\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1b868_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Event</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1b868_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1b868_row0_col0\" class=\"data row0 col0\" >2435287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b868_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1b868_row1_col0\" class=\"data row1 col0\" >1096265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:29:33,495 - INFO - Dropped columns: ['StartHesitation', 'Turn', 'Walking']\n",
      "2024-11-13 04:29:33,573 - INFO - Processed dataframe shape: (3531552, 11)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:30:50.154772Z",
     "start_time": "2024-11-12T22:30:50.143265Z"
    }
   },
   "cell_type": "code",
   "source": "df_tdcs_meta_combined = processed_df",
   "id": "ef282f525f3180b1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:30:51.112914Z",
     "start_time": "2024-11-12T22:30:51.006847Z"
    }
   },
   "cell_type": "code",
   "source": "df = df_tdcs_meta_combined.copy()",
   "id": "b59ea8fbf48d5433",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:30:53.898930Z",
     "start_time": "2024-11-12T22:30:53.875292Z"
    }
   },
   "cell_type": "code",
   "source": "df.reset_index(drop=True, inplace=True)",
   "id": "d6288053a17c4e76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:30:55.634612Z",
     "start_time": "2024-11-12T22:30:55.618760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "titles = ['AccV_mean', 'AccML_mean', 'AccAP_mean']\n",
    "default_path = 'mean_subtract/gaf_images'\n",
    "\n",
    "def create_image(df, window_no, majority_event, save_dir=default_path, no_image_creation=False):\n",
    "    \"\"\"\n",
    "    Create Gramian Angular Field images for accelerometer data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing accelerometer data\n",
    "        window_no: Window number identifier\n",
    "        majority_event: Event type for the window\n",
    "        save_dir: Directory to save images\n",
    "        no_image_creation: If True, only returns filenames without creating images\n",
    "    \n",
    "    Returns:\n",
    "        list: List of created image filenames\n",
    "    \"\"\"\n",
    "    if no_image_creation:\n",
    "        return [f\"img_{title}_{window_no}_{majority_event}.jpg\" for title in titles]\n",
    "\n",
    "    data = np.array([df['AccV_mean'].values, df['AccML_mean'].values, df['AccAP_mean'].values])\n",
    "    \n",
    "    # Create GAF transformation\n",
    "    gasf = GramianAngularField(image_size=data.shape[1])\n",
    "    gasf_transformed = gasf.fit_transform(data)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    file_names = []\n",
    "    for i, title in enumerate(titles):\n",
    "        file_name = f\"img_{title}_{window_no}_{majority_event}.jpg\"\n",
    "        file_path_jpg = os.path.join(save_dir, file_name)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(gasf_transformed[i], cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.savefig(file_path_jpg, format='jpg', dpi=200, bbox_inches='tight', pad_inches=0)\n",
    "        plt.clf()\n",
    "        plt.close(fig)\n",
    "        \n",
    "        file_names.append(file_name)\n",
    "    \n",
    "    # Clean up\n",
    "    del gasf\n",
    "    return file_names"
   ],
   "id": "67d0d3ebb3165904",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:33:16.502546Z",
     "start_time": "2024-11-12T22:32:52.478820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_windowed_data(df, window_size=4*64):\n",
    "    \"\"\"\n",
    "    Create windows from time series data with dynamic overlap.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        window_size: Size of each window in samples\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (windowed_df, label_df, window_stats)\n",
    "    \"\"\"\n",
    "    subgroups = []\n",
    "    majority_label_subgroups = []\n",
    "    \n",
    "    window_stats = {\n",
    "        'event_0': 0,\n",
    "        'event_1': 0,\n",
    "        'normal': 0,\n",
    "        'start_hesitation': 0,\n",
    "        'turn': 0,\n",
    "        'walking': 0\n",
    "    }\n",
    "    \n",
    "    window_no = 0\n",
    "    \n",
    "    for name, group in df.groupby(['Subject', 'Id', 'Visit']):\n",
    "        group_len = len(group)\n",
    "        i = 0\n",
    "        \n",
    "        while i < group_len:\n",
    "            subgroup = group.iloc[i:i + window_size].copy()\n",
    "            if len(subgroup) < window_size:\n",
    "                break\n",
    "                \n",
    "            event_counts = subgroup['Event'].value_counts()\n",
    "            count_1s = event_counts.get(1, 0)\n",
    "            count_0s = event_counts.get(0, 0)\n",
    "            majority_event = event_counts.idxmax()\n",
    "            \n",
    "            label_counts = subgroup['Label'].value_counts()\n",
    "            majority_label = label_counts.idxmax()\n",
    "            \n",
    "            # Skip mixed windows with minority FOG events\n",
    "            if count_1s > 0 and count_1s < count_0s:\n",
    "                i += window_size\n",
    "                continue\n",
    "            \n",
    "            label_subgroup = subgroup.copy()\n",
    "            \n",
    "            subgroup['Window'] = majority_event\n",
    "            label_subgroup['Window'] = majority_event\n",
    "            subgroup['Window_No'] = window_no\n",
    "            label_subgroup['Window_No'] = window_no\n",
    "            label_subgroup['Label'] = majority_label\n",
    "            \n",
    "            if majority_event == 0:\n",
    "                window_stats['event_0'] += 1\n",
    "            else:\n",
    "                window_stats['event_1'] += 1\n",
    "                \n",
    "            if majority_label == 0:\n",
    "                window_stats['normal'] += 1\n",
    "            elif majority_label == 1:\n",
    "                window_stats['start_hesitation'] += 1\n",
    "            elif majority_label == 2:\n",
    "                window_stats['turn'] += 1\n",
    "            elif majority_label == 3:\n",
    "                window_stats['walking'] += 1\n",
    "            \n",
    "            subgroups.append(subgroup)\n",
    "            majority_label_subgroups.append(label_subgroup)\n",
    "            \n",
    "            window_no += 1\n",
    "            jump_to = 2*64 if majority_event == 1 else 4*64\n",
    "            i += jump_to\n",
    "    \n",
    "    windowed_df = pd.concat(subgroups).reset_index(drop=True)\n",
    "    label_df = pd.concat(majority_label_subgroups).reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f\"Created {window_no} windows\")\n",
    "    logger.info(f\"Window statistics: {window_stats}\")\n",
    "    \n",
    "    return windowed_df, label_df, window_stats\n",
    "\n",
    "new_df, new_df_subgroups, stats = create_windowed_data(df)\n",
    "\n",
    "# Display window statistics\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "id": "c233431f7ec303e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:33:15,903 - INFO - Created 16126 windows\n",
      "2024-11-13 04:33:15,903 - INFO - Window statistics: {'event_0': 8035, 'event_1': 8091, 'normal': 8040, 'start_hesitation': 1135, 'turn': 6175, 'walking': 776}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_0: 8035\n",
      "event_1: 8091\n",
      "normal: 8040\n",
      "start_hesitation: 1135\n",
      "turn: 6175\n",
      "walking: 776\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:44:54.255248Z",
     "start_time": "2024-11-12T22:43:29.402758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NO_IMAGE_CREATION = True\n",
    "GAF_PATH = 'data/gaf_images'\n",
    "\n",
    "def create_per_window_image_ranged(\n",
    "    df, \n",
    "    chunk_size=500, \n",
    "    range_no=1, \n",
    "    file_path=GAF_PATH\n",
    "):\n",
    "    \"\"\"\n",
    "    Create GAF images for windows in specified range.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing window data\n",
    "        chunk_size: Number of windows to process at once\n",
    "        range_no: Range number (1, 2, or 3)\n",
    "        file_path: Directory to save images\n",
    "    \"\"\"\n",
    "    # Get unique windows\n",
    "    unique_windows = df['Window_No'].unique()\n",
    "    total_windows = len(unique_windows)\n",
    "    range_size = total_windows // 3\n",
    "    \n",
    "    # Calculate range boundaries\n",
    "    range_mapping = {\n",
    "        1: (0, range_size),\n",
    "        2: (range_size, 2 * range_size),\n",
    "        3: (2 * range_size, total_windows)\n",
    "    }\n",
    "    \n",
    "    if range_no not in range_mapping:\n",
    "        raise ValueError(\"Invalid range number. Must be 1, 2, or 3.\")\n",
    "    \n",
    "    start_idx, end_idx = range_mapping[range_no]\n",
    "    logger.info(f\"Processing range {range_no}: {start_idx} to {end_idx}\")\n",
    "    \n",
    "    # Get windows for the specified range\n",
    "    range_windows = unique_windows[start_idx:end_idx]\n",
    "    \n",
    "    # Process windows in chunks\n",
    "    for chunk_start in tqdm(range(0, len(range_windows), chunk_size), \n",
    "                          desc=f\"Processing range {range_no}\"):\n",
    "        # Get current chunk\n",
    "        chunk_windows = range_windows[chunk_start:chunk_start + chunk_size]\n",
    "        \n",
    "        for window_id in chunk_windows:\n",
    "            # Filter data for current window\n",
    "            filtered_df = df[df['Window_No'] == window_id]\n",
    "            event_type = filtered_df['Window'].values[0]\n",
    "            \n",
    "            # Create images\n",
    "            image_names = create_image(\n",
    "                filtered_df, \n",
    "                window_id, \n",
    "                event_type,\n",
    "                no_image_creation=NO_IMAGE_CREATION,\n",
    "                save_dir=file_path\n",
    "            )\n",
    "            \n",
    "            # Update DataFrame with image paths\n",
    "            for title, image_name in zip(titles, image_names):\n",
    "                df.loc[df['Window_No'] == window_id, f'GAF_{title}'] = image_name\n",
    "            \n",
    "        # Clean up memory\n",
    "        del filtered_df\n",
    "        \n",
    "    logger.info(f\"Completed processing range {range_no}\")\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    if NO_IMAGE_CREATION:\n",
    "        # This will only set the image file names to the dataframe\n",
    "        for i in range(1, 4):\n",
    "            new_df = create_per_window_image_ranged(new_df, range_no=i)\n",
    "        logger.info(\"Image creation completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during image creation: {str(e)}\")"
   ],
   "id": "ab2faddc8825ea35",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:43:29,520 - INFO - Processing range 3: 10750 to 16126\n",
      "Processing range 3: 100%|██████████| 11/11 [01:24<00:00,  7.70s/it]\n",
      "2024-11-13 04:44:54,239 - INFO - Completed processing range 3\n",
      "2024-11-13 04:44:54,239 - INFO - Image creation completed successfully\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:35:44.205554Z",
     "start_time": "2024-11-12T22:35:44.165929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_df(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        name: Name of output file (without .csv extension)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_path = f\"{name}.csv\"\n",
    "        output_path = os.path.join(\"data/csv\", output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Successfully saved DataFrame to {output_path}\")\n",
    "        logger.info(f\"Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving DataFrame to {name}.csv: {str(e)}\")\n",
    "        raise"
   ],
   "id": "96f1ee1de9eff69e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checkpoint 1 => Save what we computed so far",
   "id": "9705b691333d977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_df(new_df, 'windowed_tdcsfog')",
   "id": "febf9a3cce27f925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_df(new_df_subgroups, 'subgrouped_windowed_tdcsfog')",
   "id": "b45ad7a910942e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "new_df = pd.read_csv(\"windowed_tdcsfog.csv\", low_memory=False)",
   "id": "cd2caa8862f6f24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:35:50.951211Z",
     "start_time": "2024-11-12T22:35:50.795649Z"
    }
   },
   "cell_type": "code",
   "source": "new_df_copy = new_df.copy()",
   "id": "90b2f0b124583d29",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:35:58.819252Z",
     "start_time": "2024-11-12T22:35:58.792166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_mean_subtraction_columns(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str = 'Window_No',\n",
    "    target_cols: list = ['AccV', 'AccML', 'AccAP']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subtract mean values from target columns grouped by specified column.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        group_col: Column to group by\n",
    "        target_cols: List of columns to perform mean subtraction on\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added mean-subtracted columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create new column names\n",
    "        new_columns = [f\"{col}_mean\" for col in target_cols]\n",
    "        \n",
    "        # Perform mean subtraction\n",
    "        for col, new_col in zip(target_cols, new_columns):\n",
    "            mean_values = df.groupby(group_col)[col].transform('mean')\n",
    "            df[new_col] = df[col] - mean_values\n",
    "            \n",
    "        logger.info(f\"Added mean-subtracted columns: {new_columns}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in mean subtraction: {str(e)}\")\n",
    "        raise"
   ],
   "id": "50378e1d822592be",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:36:12.880993Z",
     "start_time": "2024-11-12T22:36:12.057080Z"
    }
   },
   "cell_type": "code",
   "source": "new_df_copy = add_mean_subtraction_columns(new_df_copy)",
   "id": "fd0c4ca75f308b93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:36:12,861 - INFO - Added mean-subtracted columns: ['AccV_mean', 'AccML_mean', 'AccAP_mean']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checkpoint 2 => Save for future use",
   "id": "2576a82e302b9c7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_df(new_df_copy, 'windowed_mean_subtracted_tdcsfog')",
   "id": "a652412854b3f7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checkpoint 3 => Start from here for image generation for each ranges after restart",
   "id": "18f5ce5d57dbaeef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "new_df_copy = pd.read_csv(\"data/csv/windowed_mean_subtracted_tdcsfog.csv\", low_memory=False)",
   "id": "ba2d076490d58cf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create image for range 1 to 3\n",
    "Kaggle RAM will overflow and end current session if we pass all at once. So, we will pass in a chunk.\n",
    "First pass range_no = 1, then after it is executed, restart the session. Start from **Checkpoint 3**.\n",
    "Then create images for range_no = 2. Restart the session after completion.\n",
    "Then do the same as above for range_no = 3,"
   ],
   "id": "4c3fc05fd3658b0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "create_per_window_image_ranged(new_df_copy, range_no=1)",
   "id": "c7acbfe9d4a1ce74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:44:54.437901Z",
     "start_time": "2024-11-12T22:44:54.255248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SHOW_UNIQUE_SUBJECTS = True\n",
    "\n",
    "# Get unique subjects\n",
    "if SHOW_UNIQUE_SUBJECTS:\n",
    "    try:\n",
    "        unique_subjects = new_df['Subject'].unique()\n",
    "        logger.info(f\"Found {len(unique_subjects)} unique subjects\")\n",
    "        display(unique_subjects)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting unique subjects: {str(e)}\")\n",
    "\n",
    "def split_list(data: list, test_size: float = 0.2) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Split a list into training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        data: List of items to split\n",
    "        test_size: Fraction of data to use for test set\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_data, test_data)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make copy to avoid modifying original data\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Shuffle the data\n",
    "        random.shuffle(data_copy)\n",
    "        \n",
    "        # Calculate split point\n",
    "        train_size = int((1 - test_size) * len(data_copy))\n",
    "        \n",
    "        # Split data\n",
    "        train_data = data_copy[:train_size]\n",
    "        test_data = data_copy[train_size:]\n",
    "        \n",
    "        logger.info(f\"Split sizes - Train: {len(train_data)}, Test: {len(test_data)}\")\n",
    "        return train_data, test_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error splitting data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Perform splits\n",
    "try:\n",
    "    # First split: 70/30\n",
    "    train_subjects, test_subjects = split_list(unique_subjects, test_size=0.3)\n",
    "    \n",
    "    # Second split: Split test into 20/10\n",
    "    test_subjects, valid_subjects = split_list(test_subjects, test_size=0.33)\n",
    "    \n",
    "    print(\"\\nData split results:\")\n",
    "    print(f\"Training subjects ({len(train_subjects)}): {sorted(train_subjects)}\")\n",
    "    print(f\"Testing subjects ({len(test_subjects)}): {sorted(test_subjects)}\")\n",
    "    print(f\"Validation subjects ({len(valid_subjects)}): {sorted(valid_subjects)}\")\n",
    "    \n",
    "    total = len(unique_subjects)\n",
    "    print(f\"\\nSplit percentages based on user:\")\n",
    "    print(f\"Train: {len(train_subjects)/total:.1%}\")\n",
    "    print(f\"Test: {len(test_subjects)/total:.1%}\")\n",
    "    print(f\"Valid: {len(valid_subjects)/total:.1%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in split process: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal records in dataset: {len(new_df)}\")"
   ],
   "id": "e288ce02b66ee862",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:44:54,406 - INFO - Found 62 unique subjects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['02bc69', '07285e', '082f01', '194d1d', '19ea47', '220a17',\n",
       "       '231c3b', '242a3e', '24a59d', '251738', '2a39f8', '2c98f7',\n",
       "       '2d57c2', '301ada', '312788', '31d269', '364459', '3b2403',\n",
       "       '3b2b7a', '48fd62', '4b39ac', '4ba1d3', '4bb5d0', '4ca9b3',\n",
       "       '4dc2f8', '4f13b4', '51574c', '516a67', '54ee6e', '59f492',\n",
       "       '5c0b8a', '66341b', '69cc45', '6a3e93', '743f4e', '7688c1',\n",
       "       '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', '93f49f',\n",
       "       '9f85da', 'a03db7', 'a80ae4', 'af82b2', 'b19f77', 'bc3908',\n",
       "       'c7fee4', 'c85fdf', 'c8e721', 'c95ab0', 'd8836b', 'd9312a',\n",
       "       'e39bc5', 'e8919c', 'e9fc55', 'eeaff0', 'f2c8aa', 'f62eec',\n",
       "       'f686f0', 'fa8764'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:44:54,422 - INFO - Split sizes - Train: 43, Test: 19\n",
      "2024-11-13 04:44:54,422 - INFO - Split sizes - Train: 12, Test: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split results:\n",
      "Training subjects (43): ['02bc69', '194d1d', '19ea47', '251738', '2a39f8', '2c98f7', '2d57c2', '301ada', '364459', '3b2b7a', '48fd62', '4b39ac', '4ba1d3', '4bb5d0', '4ca9b3', '4dc2f8', '4f13b4', '51574c', '54ee6e', '59f492', '5c0b8a', '66341b', '69cc45', '6a3e93', '7688c1', '79011a', '7fcee9', '87174c', '93f49f', '9f85da', 'a80ae4', 'af82b2', 'b19f77', 'c85fdf', 'c8e721', 'c95ab0', 'd8836b', 'e39bc5', 'e8919c', 'e9fc55', 'eeaff0', 'f686f0', 'fa8764']\n",
      "Testing subjects (12): ['07285e', '220a17', '231c3b', '242a3e', '24a59d', '31d269', '3b2403', '7eb666', 'a03db7', 'bc3908', 'c7fee4', 'd9312a']\n",
      "Validation subjects (7): ['082f01', '312788', '516a67', '743f4e', '8db7dd', 'f2c8aa', 'f62eec']\n",
      "\n",
      "Split percentages:\n",
      "Train: 69.4%\n",
      "Test: 19.4%\n",
      "Valid: 11.3%\n",
      "\n",
      "Total records in dataset: 4128256\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:45:46.655073Z",
     "start_time": "2024-11-12T22:45:45.450158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_data_splits(df: pd.DataFrame, train_subjects: list, valid_subjects: list, test_subjects: list) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "   \"\"\"\n",
    "   Create train, validation and test splits based on subject IDs.\n",
    "   \n",
    "   Args:\n",
    "       df: Input DataFrame\n",
    "       train_subjects: List of subjects for training\n",
    "       valid_subjects: List of subjects for validation\n",
    "       test_subjects: List of subjects for testing\n",
    "   \n",
    "   Returns:\n",
    "       tuple: (train_df, valid_df, test_df)\n",
    "   \"\"\"\n",
    "   try:\n",
    "       df_train = df[df['Subject'].isin(train_subjects)]\n",
    "       df_valid = df[df['Subject'].isin(valid_subjects)]\n",
    "       df_test = df[df['Subject'].isin(test_subjects)]\n",
    "       \n",
    "       logger.info(f\"\\nDataset split sizes:\")\n",
    "       logger.info(f\"Training set: {len(df_train):,} records\")\n",
    "       logger.info(f\"Validation set: {len(df_valid):,} records\")\n",
    "       logger.info(f\"Test set: {len(df_test):,} records\")\n",
    "       \n",
    "       # Verify no overlap\n",
    "       assert not set(df_train['Subject']).intersection(df_valid['Subject']), \"Train-Valid subject overlap found\"\n",
    "       assert not set(df_train['Subject']).intersection(df_test['Subject']), \"Train-Test subject overlap found\"\n",
    "       assert not set(df_valid['Subject']).intersection(df_test['Subject']), \"Valid-Test subject overlap found\"\n",
    "       \n",
    "       return df_train, df_valid, df_test\n",
    "       \n",
    "   except Exception as e:\n",
    "       logger.error(f\"Error creating data splits: {str(e)}\")\n",
    "       raise\n",
    "\n",
    "try:\n",
    "   df_train, df_valid, df_test = create_data_splits(\n",
    "       new_df, \n",
    "       train_subjects, \n",
    "       valid_subjects, \n",
    "       test_subjects\n",
    "   )\n",
    "   \n",
    "   total_records = len(new_df)\n",
    "   print(f\"\\nSplit proportions based on sensor data:\")\n",
    "   print(f\"Train: {len(df_train)/total_records:.1%}\")\n",
    "   print(f\"Valid: {len(df_valid)/total_records:.1%}\")\n",
    "   print(f\"Test: {len(df_test)/total_records:.1%}\")\n",
    "   \n",
    "except Exception as e:\n",
    "   logger.error(f\"Failed to create splits: {str(e)}\")"
   ],
   "id": "45c6e10a0de1f507",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 04:45:46,289 - INFO - \n",
      "Dataset split sizes:\n",
      "2024-11-13 04:45:46,289 - INFO - Training set: 3,049,984 records\n",
      "2024-11-13 04:45:46,289 - INFO - Validation set: 306,944 records\n",
      "2024-11-13 04:45:46,289 - INFO - Test set: 771,328 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split proportions:\n",
      "Train: 73.9%\n",
      "Valid: 7.4%\n",
      "Test: 18.7%\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checkpoint 4 => We are storing this so that we won't be computing this in the future.",
   "id": "f663e61bf4d60756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_df(df_train, \"df_train\")\n",
    "save_df(df_test, \"df_test\")\n",
    "save_df(df_valid, \"df_valid\")"
   ],
   "id": "a5e0236dddf7ba53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checkpoint 5 => Read already saved df_train, df_valid, df_test from folder so we won't need to recompute",
   "id": "8209bff93500f6a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\", low_memory=False)\n",
    "df_test = pd.read_csv(\"df_test.csv\", low_memory=False)\n",
    "df_valid = pd.read_csv(\"df_valid.csv\", low_memory=False)"
   ],
   "id": "997c5e16a8812e6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Move our generated images into their specific folder based on the df_train, df_test and df_valid dataset.\n",
    "For AccV the images will be moved to AccV/train, AccV/valid, and AccV/test"
   ],
   "id": "bf907263c9d11098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SOURCE_DIRECTORIES = {\n",
    "   \"AccV\": \"gaf_images/AccV\",\n",
    "   \"AccAP\": \"gaf_images/AccAP\",\n",
    "   \"AccML\": \"gaf_images/AccML\"\n",
    "}\n",
    "\n",
    "def move_to_specific_dir(df_local: pd.DataFrame, directories: dict) -> None:\n",
    "   \"\"\"\n",
    "   Move GAF images to their respective directories based on acceleration type.\n",
    "   \n",
    "   Args:\n",
    "       df_local: DataFrame containing window information\n",
    "       directories: Dictionary mapping acceleration types to target directories\n",
    "   \"\"\"\n",
    "   try:\n",
    "       # Create target directories\n",
    "       for folder_name, path in directories.items():\n",
    "           os.makedirs(path, exist_ok=True)\n",
    "           logger.info(f\"Created directory: {path}\")\n",
    "       \n",
    "       total_copied = 0\n",
    "       for cur_window_no in tqdm(df_local['Window_No'].unique(), desc=\"Processing windows\"):\n",
    "           window_str = f\"_{cur_window_no}_\"\n",
    "           \n",
    "           for folder_name, source_dir in SOURCE_DIRECTORIES.items():\n",
    "               if not os.path.exists(source_dir):\n",
    "                   logger.warning(f\"Source directory not found: {source_dir}\")\n",
    "                   continue\n",
    "                   \n",
    "               target_file = f\"{folder_name}_mean{window_str}\"\n",
    "               \n",
    "               for filename in os.listdir(source_dir):\n",
    "                   if target_file in filename:\n",
    "                       source_file = os.path.join(source_dir, filename)\n",
    "                       destination_file = os.path.join(directories[folder_name], filename)\n",
    "                       \n",
    "                       shutil.copy2(source_file, destination_file)\n",
    "                       total_copied += 1\n",
    "       \n",
    "       logger.info(f\"Successfully copied {total_copied} files to their respective directories\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       logger.error(f\"Error during file movement: {str(e)}\")\n",
    "       raise\n",
    "\n",
    "train_directories = {\n",
    "   \"AccV\": \"gaf_images/AccV/train\",\n",
    "   \"AccAP\": \"gaf_images/AccAP/train\",\n",
    "   \"AccML\": \"gaf_images/AccML/train\"\n",
    "}\n",
    "\n",
    "valid_directories = {\n",
    "   \"AccV\": \"gaf_images/AccV/valid\",\n",
    "   \"AccAP\": \"gaf_images/AccAP/valid\",\n",
    "   \"AccML\": \"gaf_images/AccML/valid\"\n",
    "}\n",
    "\n",
    "test_directories = {\n",
    "   \"AccV\": \"gaf_images/AccV/test\",\n",
    "   \"AccAP\": \"gaf_images/AccAP/test\",\n",
    "   \"AccML\": \"gaf_images/AccML/test\"\n",
    "}\n",
    "\n",
    "try:\n",
    "   # Move files to their respective directories\n",
    "   for split_name, df_split, dirs in [\n",
    "       (\"train\", df_train, train_directories),\n",
    "       (\"valid\", df_valid, valid_directories),\n",
    "       (\"test\", df_test, test_directories)\n",
    "   ]:\n",
    "       logger.info(f\"\\nProcessing {split_name} split...\")\n",
    "       move_to_specific_dir(df_split, dirs)\n",
    "       \n",
    "except Exception as e:\n",
    "   logger.error(f\"Failed to move files: {str(e)}\")"
   ],
   "id": "3da970c48a9c9a1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:52:26.676271Z",
     "start_time": "2024-11-12T22:52:25.267056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_directories():\n",
    "    \"\"\"\n",
    "    Analyze distribution of images across train/test/valid splits and classes.\n",
    "    \"\"\"\n",
    "    parent_dirs = ['AccV', 'AccAP', 'AccML']\n",
    "    splits = ['train', 'test', 'valid']\n",
    "    base_path = 'data'\n",
    "    \n",
    "    for parent in parent_dirs:\n",
    "        print(f\"\\n=== Analysis for {parent} ===\")\n",
    "        \n",
    "        total_images = 0\n",
    "        split_counts = {}\n",
    "        class_counts = {split: {0: 0, 1: 0} for split in splits}\n",
    "        \n",
    "        for split in splits:\n",
    "            path = os.path.join(base_path, parent, split)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Warning: Path does not exist: {path}\")\n",
    "                continue\n",
    "                \n",
    "            # Count images in this split\n",
    "            files = os.listdir(path)\n",
    "            split_counts[split] = len(files)\n",
    "            total_images += len(files)\n",
    "            \n",
    "            # Count classes\n",
    "            for file in files:\n",
    "                # Extract class from filename (last digit before .jpg)\n",
    "                try:\n",
    "                    class_label = int(file.split('_')[-1].split('.')[0])\n",
    "                    class_counts[split][class_label] += 1\n",
    "                except:\n",
    "                    print(f\"Warning: Could not parse class from filename: {file}\")\n",
    "        \n",
    "        # Print split proportions\n",
    "        print(\"\\nSplit Proportions:\")\n",
    "        for split, count in split_counts.items():\n",
    "            proportion = (count / total_images * 100) if total_images > 0 else 0\n",
    "            print(f\"{split}: {count} images ({proportion:.1f}%)\")\n",
    "            \n",
    "        # Print class distributions\n",
    "        print(\"\\nClass Distributions:\")\n",
    "        for split in splits:\n",
    "            if split in class_counts:\n",
    "                total_split = sum(class_counts[split].values())\n",
    "                print(f\"\\n{split}:\")\n",
    "                for class_label, count in class_counts[split].items():\n",
    "                    proportion = (count / total_split * 100) if total_split > 0 else 0\n",
    "                    print(f\"Class {class_label}: {count} images ({proportion:.1f}%)\")\n",
    "\n",
    "# Run analysis\n",
    "try:\n",
    "    analyze_directories()\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {str(e)}\")"
   ],
   "id": "27ad49ae6221b7dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis for AccV ===\n",
      "\n",
      "Split Proportions:\n",
      "train: 8288 images (48.9%)\n",
      "test: 6255 images (36.9%)\n",
      "valid: 2410 images (14.2%)\n",
      "\n",
      "Class Distributions:\n",
      "\n",
      "train:\n",
      "Class 0: 5106 images (61.6%)\n",
      "Class 1: 3182 images (38.4%)\n",
      "\n",
      "test:\n",
      "Class 0: 2052 images (32.8%)\n",
      "Class 1: 4203 images (67.2%)\n",
      "\n",
      "valid:\n",
      "Class 0: 1700 images (70.5%)\n",
      "Class 1: 710 images (29.5%)\n",
      "\n",
      "=== Analysis for AccAP ===\n",
      "\n",
      "Split Proportions:\n",
      "train: 8288 images (48.9%)\n",
      "test: 6255 images (36.9%)\n",
      "valid: 2410 images (14.2%)\n",
      "\n",
      "Class Distributions:\n",
      "\n",
      "train:\n",
      "Class 0: 5106 images (61.6%)\n",
      "Class 1: 3182 images (38.4%)\n",
      "\n",
      "test:\n",
      "Class 0: 2052 images (32.8%)\n",
      "Class 1: 4203 images (67.2%)\n",
      "\n",
      "valid:\n",
      "Class 0: 1700 images (70.5%)\n",
      "Class 1: 710 images (29.5%)\n",
      "\n",
      "=== Analysis for AccML ===\n",
      "\n",
      "Split Proportions:\n",
      "train: 8288 images (48.9%)\n",
      "test: 6255 images (36.9%)\n",
      "valid: 2410 images (14.2%)\n",
      "\n",
      "Class Distributions:\n",
      "\n",
      "train:\n",
      "Class 0: 5106 images (61.6%)\n",
      "Class 1: 3182 images (38.4%)\n",
      "\n",
      "test:\n",
      "Class 0: 2052 images (32.8%)\n",
      "Class 1: 4203 images (67.2%)\n",
      "\n",
      "valid:\n",
      "Class 0: 1700 images (70.5%)\n",
      "Class 1: 710 images (29.5%)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AFTER THIS IS DONE.... WE CAN MOVE TO TRAINING OUR MULTI CHANNEL CNN MODEL",
   "id": "26f2630b425764e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
